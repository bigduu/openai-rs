//! Contains type definitions specific to the OpenAI API,
//! particularly for chat completions and streaming responses.

use derive_builder::Builder;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

// --- Common Structures ---

/// Represents a tool call requested by the model.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ToolCall {
    /// The ID of the tool call.
    pub id: String,
    /// The type of the tool. Currently, only `function` is supported.
    pub r#type: String, // Should be "function"
    /// The function that the model wants to call.
    pub function: FunctionCall,
}

/// Represents the function details within a tool call.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct FunctionCall {
    /// The name of the function to call.
    pub name: String,
    /// The arguments to call the function with, as generated by the model in JSON format.
    /// Note that the model does not always generate valid JSON, and may hallucinate parameters
    /// not defined by your function schema. Validate the arguments in your code before calling
    /// your function.
    pub arguments: String, // JSON string
}

/// Represents a tool definition provided by the user.
#[derive(Debug, Clone, Serialize, PartialEq)]
pub struct Tool {
    /// The type of the tool. Currently, only `function` is supported.
    pub r#type: String, // Should be "function"
    /// The function definition.
    pub function: FunctionDefinition,
}

/// Represents the definition of a function tool.
#[derive(Debug, Clone, Serialize, PartialEq)]
pub struct FunctionDefinition {
    /// A description of what the function does, used by the model to choose when and how to call the function.
    pub description: Option<String>,
    /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    pub name: String,
    /// The parameters the functions accepts, described as a JSON Schema object.
    /// See the [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for examples, and the
    /// [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
    /// To describe a function with no parameters, provide the value `{"type": "object", "properties": {}}`.
    pub parameters: serde_json::Value, // JSON Schema object
}

/// Controls which (if any) function is called by the model.
#[derive(Debug, Clone, Serialize, PartialEq)]
#[serde(untagged)] // Allows serialization as either a string or a specific function object
pub enum ToolChoice {
    /// The model will not call a function and instead generates a message.
    #[serde(rename = "none")]
    None,
    /// The model can pick between generating a message or calling a function.
    #[serde(rename = "auto")]
    Auto,
    /// The model must call one or more functions.
    #[serde(rename = "required")]
    Required,
    /// Forces the model to call the specified function.
    Function {
        /// The type of the tool. Currently, only `function` is supported.
        #[serde(rename = "type")]
        r#type: String, // Should always be "function"
        /// The name of the function to call.
        function: FunctionName,
    },
}

/// Helper struct for specifying a function name within ToolChoice.
#[derive(Debug, Clone, Serialize, PartialEq)]
pub struct FunctionName {
    pub name: String,
}

/// Represents the format of the response.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ResponseFormat {
    /// Must be one of `text` or `json_object`.
    pub r#type: String, // "text" or "json_object"
}

// --- Request Structures ---

/// Represents a single message in the chat conversation history.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Default, Builder)]
#[builder(setter(strip_option), default)]
pub struct OpenAiChatMessage {
    /// The role of the author of this message. One of `system`, `user`, `assistant`, or `tool`.
    #[builder(setter(into))] // Mandatory field
    pub role: String,
    /// The contents of the message. `content` is required for all messages except assistant messages with function calls.
    #[builder(setter(into))]
    pub content: Option<String>,
    /// The name of the author of this message. `name` is required if role is `function`, and it should be the name of the
    /// function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub name: Option<String>,
    /// The tool calls generated by the model, such as function calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tool_calls: Option<Vec<ToolCall>>,
    /// Tool call that this message is responding to. Required if role is `tool`.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tool_call_id: Option<String>,
}

/// Represents the request body for the OpenAI Chat Completions API.
#[derive(Debug, Clone, Serialize, Default, Builder)]
#[builder(setter(strip_option), default)] // Apply to all Option fields
pub struct OpenAiChatCompletionRequest {
    /// ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)
    /// table for details on which models work with the Chat API.
    #[builder(setter(into))] // Mandatory field
    pub model: String,
    /// A list of messages comprising the conversation so far.
    #[builder(setter(into))] // Mandatory field
    pub messages: Vec<OpenAiChatMessage>,
    /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
    /// [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
    /// as they become available, with the stream terminated by a `data: [DONE]` message.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency
    /// in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,
    /// Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens
    /// (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub logit_bias: Option<HashMap<String, i32>>,
    /// Whether to return log probabilities of the output tokens, if `logprobs` is true.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<bool>,
    /// An integer between 0 and 5 specifying the number of most likely tokens to return at each token position,
    /// each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u32>,
    /// The maximum number of [tokens](/tokenizer) to generate in the chat completion.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,
    /// How many chat completion choices to generate for each input message. Note that you will be charged based
    /// on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<u32>, // Typically 1
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,
    /// An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<ResponseFormat>,
    /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically,
    /// such that repeated requests with the same `seed` and parameters should return the same result.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,
    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub stop: Option<Vec<String>>,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of
    /// the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tools: Option<Vec<Tool>>,
    /// Controls which (if any) function is called by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub user: Option<String>,
}

// --- Response Structures (Standard & Streaming) ---

/// Represents the overall response for a non-streaming chat completion.
#[derive(Debug, Clone, Deserialize)]
pub struct OpenAiChatCompletionResponse {
    /// A unique identifier for the chat completion.
    pub id: String,
    /// A list of chat completion choices. Can be more than one if `n` is greater than 1.
    pub choices: Vec<CompletionChoice>,
    /// The Unix timestamp (in seconds) of when the chat completion was created.
    pub created: u64,
    /// The model used for the chat completion.
    pub model: String,
    /// This fingerprint represents the backend configuration that the model runs with.
    pub system_fingerprint: Option<String>,
    /// The object type, which is always `chat.completion`.
    pub object: String, // "chat.completion"
    /// Usage statistics for the completion request.
    pub usage: Option<CompletionUsage>,
}

/// Represents a choice within a non-streaming completion response.
#[derive(Debug, Clone, Deserialize)]
pub struct CompletionChoice {
    /// The reason the model stopped generating tokens.
    pub finish_reason: String, // e.g., "stop", "length", "tool_calls", "content_filter"
    /// The index of the choice in the list of choices.
    pub index: u32,
    /// A chat completion message generated by the model.
    pub message: OpenAiChatMessage,
    /// Log probability information for the choice.
    pub logprobs: Option<LogProbs>,
}

/// Represents usage statistics for the completion request.
#[derive(Debug, Clone, Deserialize)]
pub struct CompletionUsage {
    /// Number of tokens in the generated completion.
    pub completion_tokens: u32,
    /// Number of tokens in the prompt.
    pub prompt_tokens: u32,
    /// Total number of tokens used in the request (prompt + completion).
    pub total_tokens: u32,
}

/// Represents log probability information.
#[derive(Debug, Clone, Deserialize)]
pub struct LogProbs {
    /// A list of message content tokens with log probability information.
    pub content: Option<Vec<TokenLogProb>>,
}

/// Represents a single token's log probability information.
#[derive(Debug, Clone, Deserialize)]
pub struct TokenLogProb {
    /// The token.
    pub token: String,
    /// The log probability of this token.
    pub logprob: f64,
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null`.
    pub bytes: Option<Vec<u8>>,
    /// List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than `top_logprobs` returned.
    pub top_logprobs: Vec<TopLogProb>,
}

/// Represents one of the most likely tokens and its log probability.
#[derive(Debug, Clone, Deserialize)]
pub struct TopLogProb {
    /// The token.
    pub token: String,
    /// The log probability of this token.
    pub logprob: f64,
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null`.
    pub bytes: Option<Vec<u8>>,
}

/// Represents a single chunk received in a streaming chat completion response.
#[derive(Debug, Clone, Deserialize)]
pub struct OpenAiStreamChunk {
    /// A unique identifier for the chat completion chunk.
    pub id: String,
    /// A list of chat completion choices. Can contain more than one if `n` is greater than 1.
    pub choices: Vec<StreamChoice>,
    /// The Unix timestamp (in seconds) of when the chat completion chunk was created.
    pub created: u64,
    /// The model to generate the completion.
    pub model: String,
    /// The object type, which is always `chat.completion.chunk`.
    pub object: String, // "chat.completion.chunk"
    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    pub system_fingerprint: Option<String>,
    /// An optional field that appears only in the very last chunk of the stream.
    pub usage: Option<CompletionUsage>,
}

/// Represents a choice within a stream chunk.
#[derive(Debug, Clone, Deserialize)]
pub struct StreamChoice {
    /// A chat completion delta generated by streamed model responses.
    pub delta: StreamDelta,
    /// The reason the model stopped generating tokens. This will be non-null only in the last chunk of the stream.
    pub finish_reason: Option<String>, // e.g., "stop", "length", "tool_calls", "content_filter"
    /// The index of the choice in the list of choices.
    pub index: u32,
    /// Log probability information for the choice.
    pub logprobs: Option<LogProbs>,
}

/// Represents the delta (change) in content for a stream choice.
#[derive(Debug, Clone, Deserialize)]
pub struct StreamDelta {
    /// The contents of the chunk message.
    pub content: Option<String>,
    /// The tool calls generated by the model, such as function calls.
    #[serde(default)] // Ensure tool_calls defaults to None if missing
    pub tool_calls: Option<Vec<StreamToolCall>>,
    /// The role of the author of this message.
    pub role: Option<String>, // Usually "assistant", present only in the first delta for a choice
}

/// Represents a tool call within a stream delta.
#[derive(Debug, Clone, Deserialize, PartialEq)]
pub struct StreamToolCall {
    pub index: u32,
    /// The ID of the tool call.
    pub id: Option<String>,
    /// The type of the tool. Currently, only `function` is supported.
    pub r#type: Option<String>, // Should be "function"
    /// The function that the model wants to call.
    pub function: Option<StreamFunctionCall>,
}

/// Represents the function details within a streamed tool call delta.
/// Arguments are streamed as partial JSON.
#[derive(Debug, Clone, Deserialize, PartialEq)]
pub struct StreamFunctionCall {
    /// The name of the function to call.
    pub name: Option<String>,
    /// The arguments to call the function with, as generated by the model in JSON format delta.
    pub arguments: Option<String>, // Partial JSON string
}
