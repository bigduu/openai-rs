use super::common::{CompletionUsage, LogProbs, ResponseFormat};
use super::function::{Tool, ToolCall, ToolChoice};
use derive_builder::Builder;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Represents a single message in the chat conversation history.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Default, Builder)]
#[builder(setter(strip_option), default)]
pub struct OpenAiChatMessage {
    /// The role of the author of this message.
    #[builder(setter(into))]
    pub role: String,
    /// The contents of the message.
    #[builder(setter(into))]
    pub content: Option<String>,
    /// The name of the author of this message.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub name: Option<String>,
    /// The tool calls generated by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tool_calls: Option<Vec<ToolCall>>,
    /// Tool call that this message is responding to.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tool_call_id: Option<String>,
}

/// Represents the request body for the OpenAI Chat Completions API.
#[derive(Debug, Clone, Serialize, Deserialize, Default, Builder)]
#[builder(setter(strip_option), default)]
pub struct OpenAiChatCompletionRequest {
    /// ID of the model to use.
    #[builder(setter(into))]
    pub model: String,
    /// A list of messages comprising the conversation so far.
    #[builder(setter(into))]
    pub messages: Vec<OpenAiChatMessage>,
    /// If set, partial message deltas will be sent.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    /// Number between -2.0 and 2.0 for frequency penalty.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,
    /// Modify the likelihood of specified tokens appearing in the completion.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub logit_bias: Option<HashMap<String, i32>>,
    /// Whether to return log probabilities of the output tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<bool>,
    /// Number of most likely tokens to return at each token position.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u32>,
    /// The maximum number of tokens to generate.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,
    /// How many chat completion choices to generate.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<u32>,
    /// Number between -2.0 and 2.0 for presence penalty.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,
    /// An object specifying the format that the model must output.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<ResponseFormat>,
    /// Seed for deterministic sampling.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<i64>,
    /// Up to 4 sequences where the API will stop generating further tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub stop: Option<Vec<String>>,
    /// What sampling temperature to use.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    /// An alternative to sampling with temperature.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    /// A list of tools the model may call.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub tools: Option<Vec<Tool>>,
    /// Controls which (if any) function is called by the model.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,
    /// A unique identifier representing your end-user.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[builder(setter(into))]
    pub user: Option<String>,
}

/// Represents the overall response for a non-streaming chat completion.
#[derive(Debug, Clone, Deserialize)]
pub struct OpenAiChatCompletionResponse {
    /// A unique identifier for the chat completion.
    pub id: String,
    /// A list of chat completion choices.
    pub choices: Vec<CompletionChoice>,
    /// The Unix timestamp of when the chat completion was created.
    pub created: u64,
    /// The model used for the chat completion.
    pub model: String,
    /// This fingerprint represents the backend configuration.
    pub system_fingerprint: Option<String>,
    /// The object type.
    pub object: String, // "chat.completion"
    /// Usage statistics for the completion request.
    pub usage: Option<CompletionUsage>,
}

/// Represents a choice within a non-streaming completion response.
#[derive(Debug, Clone, Deserialize)]
pub struct CompletionChoice {
    /// The reason the model stopped generating tokens.
    pub finish_reason: String,
    /// The index of the choice in the list of choices.
    pub index: u32,
    /// A chat completion message generated by the model.
    pub message: OpenAiChatMessage,
    /// Log probability information for the choice.
    pub logprobs: Option<LogProbs>,
}
